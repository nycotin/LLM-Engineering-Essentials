{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nycotin/LLM-Engineering-Essentials/blob/main/topic1/1.2_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon.\n",
        "\n",
        "# 1.2. Tokenization"
      ],
      "metadata": {
        "id": "Vm506vpf9u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going into the LLM, the prompt is **tokenized**, that is, split into **tokens**. And the completion is also generated **token by token**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nKMwN3nZ9ZoEQcdzhxmv5QPXLHv6K7MK\" width=600 />\n",
        "</center>\n",
        "\n",
        "As you see, the words are cut into pieces called **subword units**. Why? This allows to:\n",
        "\n",
        "- Have a dictionary (list of all tokens) of fixes size. This wouldn't work with **word-level** tokeinization: typos and neologisms would make the dictionary too huge.\n",
        "- Have meaningful tokens, unlike in **character-level** tokenization.\n",
        "\n",
        "**Note**. Some LLMs use hybrid strategy, where `import pandas as pd` may be one token."
      ],
      "metadata": {
        "id": "0p-CFy9Q9u9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the OpenAI tokenizer. For that, we need the `tiktoken` library."
      ],
      "metadata": {
        "id": "ih6xPadl9u9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "_nULlfhm9u9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')"
      ],
      "metadata": {
        "id": "sGmJA7Ln9u9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's tokenize a simple prompt."
      ],
      "metadata": {
        "id": "pyEcD4od9u9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = 'Darth Vader was born on Tatooine.'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Tokenization\n",
        "print('Decoding back: ', encoding.decode(encoded_string))\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: \"{encoding.decode([token])}\"')"
      ],
      "metadata": {
        "id": "YbwNvJ9O9u9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put the tokens in quotes to make it more clear that some tokens have a space before them and some do not. So, `\" D\"` and `\"D\"` will be different tokens; the first one used after a space and the second one in the beginning of a text or in the middle of a word. For example:"
      ],
      "metadata": {
        "id": "k4Epyw1m-hG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = 'D D!D'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Tokenization\n",
        "print('Decoding back: ', encoding.decode(encoded_string))\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: \"{encoding.decode([token])}\"')"
      ],
      "metadata": {
        "id": "rgjbSu9Y-5e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may also be interesting to see how the model splits code:"
      ],
      "metadata": {
        "id": "dkg9eMye9u9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_code = \"\"\"import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.hist(df['values'], bins=5, edgecolor='black')\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Values')\n",
        "plt.show()\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(python_code)\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string[:40]:\n",
        "    print(f'{token}: {encoding.decode([token])}')"
      ],
      "metadata": {
        "id": "Fo1wclqG9u9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the tokenizer of an open-source LLM\n",
        "\n",
        "Most open-source LLMs are available at [Hugging Face](https://huggingface.co/). For example, here is the page of the Qwen2.5-Coder-32B model: [link](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct).\n",
        "\n",
        "Usually, a model page contains demo code showcasing how to use this model. Here we only need its tokenizer, so we deleted everything else."
      ],
      "metadata": {
        "id": "xplrLBJF_lwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Darth Vader was born on Tatooine.\"\n",
        "\n",
        "tokenizer.tokenize(prompt)"
      ],
      "metadata": {
        "id": "C66qxnLo_jZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the tokenization is different from what you've seen for gpt-4o-mini. Also, a symbol `Ä ` is used instead of a space."
      ],
      "metadata": {
        "id": "N3FxqylbBe9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note.** Unlike Qwen, many models will actually ask you to login to Hugging Face and consent to their terms of use before you can get your hands on them.\n",
        "\n",
        "After that, you'll need to:\n",
        "\n",
        "1. [Get an access token](https://huggingface.co/settings/tokens) with **write** permissions. Make sure to copy and save it; you won't be able to see it again.\n",
        "\n",
        "2. Login to Hugging Face from Jupyter, for example, using\n",
        "\n",
        "```\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "```\n",
        "\n",
        "(Make sure to enter the access token you've created before)"
      ],
      "metadata": {
        "id": "vOudnN-VA9NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "AqGFb4ByDA1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and character-level tasks\n",
        "\n",
        "LLMs don't naturally see the text on a character level, and it is one of the reasons why LLMs are not so good at arithmetics, reversing strings and similar tasks.\n",
        "\n",
        "As an example, let's investigate how LLMs and their tokenizers cope with numbers and arithmetics."
      ],
      "metadata": {
        "id": "HsiST0O_FPnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_string = '12345678'\n",
        "\n",
        "# Tokenization\n",
        "encoded_string = encoding.encode(initial_string)\n",
        "print('After tokenization (encoded string): ', encoded_string)\n",
        "\n",
        "# Decoding each token:\n",
        "for token in encoded_string:\n",
        "    print(f'{token}: {encoding.decode([token])}')"
      ],
      "metadata": {
        "id": "DXYS5lC59u9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, LLM doesn't see a large number as a sequence of characters. Instead, it operates with larger chunks of digits. This inevitable makes arithmetics harder for the LLM; just imagine the size of a \"mental multiplication table\" it should be aware of!\n",
        "\n",
        "Let's demonstrate this with an example:"
      ],
      "metadata": {
        "id": "h7tmCurKJQtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "WqCgRtIRIcN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "NRpRGdl5IdJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You're an expert in floating point computations.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"7.24*19.13 =\"\"\"\n",
        "    },\n",
        "    ],\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "YTlC-5omIVOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is $138.5012$. The error is not large, but overall, don't put too much trust into LLM calculations."
      ],
      "metadata": {
        "id": "x5b0aEGFI-oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: Further exploration of the tokenizers\n",
        "\n",
        "In this task we'll continue experimenting with LLM tokenizers to better understand their properties and restrictions.\n",
        "\n",
        "Let's make sure that we have the `tiktoken` library installed:"
      ],
      "metadata": {
        "id": "ar9xouzsMfdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMtKJTBmKScJ"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by looking at some particular tokens.\n",
        "\n",
        "**Task 1.**. Print the subword unit (a string) corresponding to the token index 10101.\n",
        "\n",
        "Food for thought:\n",
        "Try to guess why do we have this strange string among tokens. It should have been quite frequent in the training data to appear as a token."
      ],
      "metadata": {
        "id": "R_H15pEhSwQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate the code for printing the subword unit (a string) corresponding to the token index 10101 with the tiktoken library.\n",
        "\n",
        "print(encoding.decode([10101]))\n"
      ],
      "metadata": {
        "id": "nKZ8L646drGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the vocabulary size of GPT-4o. Web search can quickly give us the number 200k, but we want to be sure.\n",
        "\n",
        "If we explore the `encoding` object a bit more deeply, we will find:\n",
        "\n",
        "- `encoding._mergeable_ranks` contains regular `token:index` dictionary; its size is 199998.\n",
        "- `encoding._special_tokens` contains two special tokens:"
      ],
      "metadata": {
        "id": "fovDi3L5SGch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding._special_tokens"
      ],
      "metadata": {
        "id": "0sTijh6cWA9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, indeed we have 200k items in the dictionary."
      ],
      "metadata": {
        "id": "ZtsGS5KBo_x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.** LLM's dictionary, that is the list of all subwords it uses as tokens, is trained on common crawl web data. How exactly it is done, you'll learn further in the course. Right now, we'll observe how the training set composition influences the behaviour of tokenization for different languages.\n",
        "\n",
        "For this task, we've chosen four languages: English, French, Swahili, and Traditional Chinese. We generated a short story in Simple English about peasants and animals, and then we translated it to other languages using Google translate. Thus we've got a dictionary of four strings:\n",
        "```\n",
        "excerpts = {\n",
        "    'English': ...,\n",
        "    'French': ...,\n",
        "    'Swahili': ...,\n",
        "    'Chinese': ...\n",
        "}\n",
        "```\n",
        "\n",
        "Find the length of tokenized sequences for each of these strings. Observe the difference.\n",
        "\n",
        "**Food for thought.** Why did we choose such a naive topic for the short story? Why not adventures of a Little Brave Python in the world of Machine Learning? Why not just take the wikipedia article about Jacques-Louis David? Hint: that would damage the purity of our experiment.\n",
        "\n",
        "The data is:"
      ],
      "metadata": {
        "id": "J_0dIvz-O3yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excerpts = {\n",
        "    'English': '''\n",
        "Once upon a time, in a small village, there lived some kind peasants. They worked hard every day, taking care of their fields and animals. The peasants had cows, chickens, goats, and a big brown dog named Buddy.\n",
        "\n",
        "Every morning, the peasants woke up early. They fed the chickens, who clucked happily. They milked the cows, who gave them fresh, sweet milk. The goats liked to jump and play, making everyone laugh. Buddy, the dog, helped keep the animals safe.\n",
        "\n",
        "One day, a storm came. The wind blew hard, and the rain fell heavily. The peasants quickly brought all the animals into the big barn. Inside, it was warm and dry. The cows lay down on the soft hay. The chickens snuggled together on their perch. The goats found a corner to rest, and Buddy lay by the door, watching over everyone.\n",
        "\n",
        "The storm lasted all night, but the peasants and their animals were safe in the barn. In the morning, the sun came out, and the storm was gone. The peasants opened the barn doors, and the animals went outside, happy and free.\n",
        "\n",
        "The peasants fixed the fences and cleaned the barn. They worked together, helping each other and their animals. At the end of the day, they sat under a big tree, watching the sunset. Buddy lay at their feet, and the animals grazed nearby.\n",
        "\n",
        "The village was peaceful, and the peasants were happy. They knew that as long as they took care of their animals and each other, they would always have a good life.\n",
        "\n",
        "And so, they lived happily ever after.\n",
        "''',\n",
        "    'French': '''\n",
        "Il Ã©tait une fois, dans un petit village, de gentils paysans. Ils travaillaient dur chaque jour, prenant soin de leurs champs et de leurs animaux. Les paysans avaient des vaches, des poules, des chÃ¨vres et un gros chien brun nommÃ© Buddy.\n",
        "\n",
        "Chaque matin, les paysans se rÃ©veillaient tÃ´t. Ils nourrissaient les poules qui glousaient joyeusement. Ils traitaient les vaches qui leur donnaient du lait frais et sucrÃ©. Les chÃ¨vres aimaient sauter et jouer, faisant rire tout le monde. Buddy, le chien, a aidÃ© Ã  assurer la sÃ©curitÃ© des animaux.\n",
        "\n",
        "Un jour, une tempÃªte est arrivÃ©e. Le vent soufflait fort et la pluie tombait abondamment. Les paysans ont rapidement amenÃ© tous les animaux dans la grande grange. Ã lâintÃ©rieur, il faisait chaud et sec. Les vaches se couchent sur le foin moelleux. Les poules se blottissaient les unes contre les autres sur leur perchoir. Les chÃ¨vres trouvÃ¨rent un coin pour se reposer et Buddy resta allongÃ© prÃ¨s de la porte, veillant sur tout le monde.\n",
        "\n",
        "La tempÃªte a durÃ© toute la nuit, mais les paysans et leurs animaux Ã©taient en sÃ©curitÃ© dans la grange. Le matin, le soleil s'est levÃ© et la tempÃªte s'est calmÃ©e. Les paysans ouvrirent les portes de la grange et les animaux sortirent, heureux et libres.\n",
        "\n",
        "Les paysans rÃ©parÃ¨rent les clÃ´tures et nettoyÃ¨rent la grange. Ils ont travaillÃ© ensemble, s'entraidant ainsi que leurs animaux. Ã la fin de la journÃ©e, ils Ã©taient assis sous un grand arbre et regardaient le coucher du soleil. Buddy gisait Ã  leurs pieds et les animaux paissaient Ã  proximitÃ©.\n",
        "\n",
        "Le village Ã©tait paisible et les paysans Ã©taient heureux. Ils savaient que tant quâils prendraient soin de leurs animaux et les uns des autres, ils auraient toujours une belle vie.\n",
        "\n",
        "Et ainsi, ils vÃ©curent heureux pour toujours.\n",
        "''',\n",
        "  'Swahili': '''\n",
        "Hapo zamani za kale, katika kijiji kidogo waliishi wakulima wa aina fulani. Walifanya kazi kwa bidii kila siku, wakitunza mashamba na wanyama wao. Wakulima walikuwa na ng'ombe, kuku, mbuzi, na mbwa mkubwa wa kahawia aliyeitwa Buddy.\n",
        "\n",
        "Kila asubuhi, wakulima waliamka mapema. Walilisha kuku, ambao waliruka kwa furaha. Wakakamua ng'ombe, ambaye aliwapa maziwa safi, matamu. Mbuzi walipenda kuruka na kucheza, na kufanya kila mtu acheke. Buddy, mbwa, alisaidia kuweka wanyama salama.\n",
        "\n",
        "Siku moja, dhoruba ilikuja. Upepo ulivuma kwa nguvu, na mvua ikanyesha sana. Wakulima haraka wakaleta wanyama wote kwenye zizi kubwa. Ndani, kulikuwa na joto na kavu. Ng'ombe walilala kwenye nyasi laini. Kuku walijibana kwenye sangara wao. Mbuzi walipata kona ya kupumzika, na Buddy akalala karibu na mlango, akiangalia kila mtu.\n",
        "\n",
        "Dhoruba hiyo ilidumu usiku kucha, lakini wakulima na wanyama wao walikuwa salama ghalani. Asubuhi, jua lilitoka, na dhoruba ikatoweka. Wakulima walifungua milango ya ghalani, na wanyama wakatoka nje, wakiwa na furaha na huru.\n",
        "\n",
        "Wakulima walitengeneza ua na kusafisha ghala. Walifanya kazi pamoja, kusaidiana na wanyama wao. Mwisho wa siku walikaa chini ya mti mkubwa wakitazama machweo ya jua. Rafiki alilala miguuni mwao, na wanyama walilisha karibu.\n",
        "\n",
        "Kijiji kilikuwa na amani, na wakulima walikuwa na furaha. Walijua kwamba maadamu wanatunza wanyama wao na kila mmoja wao, wangekuwa na maisha mazuri kila wakati.\n",
        "\n",
        "Na hivyo, waliishi kwa furaha milele.\n",
        "''',\n",
        "  'Chinese': '''\n",
        "å¾åï¼å¨ä¸åå°æèè£¡ï¼ä½èä¸äºåè¯çè¾²å¤«ãä»åæ¯å¤©åªåå·¥ä½ï¼ç§é¡§ä»åçç°å°ååç©ãè¾²æ°é¤äºçãéãå±±ç¾åä¸é»åå«å·´è¿ªçæ£è²å¤§çã\n",
        "\n",
        "æ¯å¤©æ©ä¸ï¼è¾²æ°åé½æ©æ©èµ·åºãä»åé¤µéï¼éé«èå°å¯å¯å«ãä»åçµ¦ä¹³çæ å¥¶ï¼ä¹³ççµ¦å®åæ°é®®ãçççå¥¶ãå±±ç¾ååæ­¡è·³ä¾è·³å»ï¼ç©èï¼éå¾å¤§å®¶ååå¤§ç¬ãå·´è¿ªçå¹«å©ä¿è­·åç©çå®å¨ã\n",
        "\n",
        "æä¸å¤©ï¼ä¸å ´æ´é¢¨é¨ä¾äºãé¢¨å®å¾å¾å¤§ï¼é¨ä¹ä¸å¾å¾å¤§ãè¾²å¤«å¾å¿«å°±æææçç²çé½å¸¶é²äºå¤§ç©åãè£¡é¢æº«æèä¹¾ç¥ãå¥¶çèººå¨æè»çä¹¾èä¸ãéåå¨æ£²æ¨ä¸ä¾åå¨ä¸èµ·ãå±±ç¾åæ¾å°äºä¸åè§è½ä¼æ¯ï¼å·´è¿ªèººå¨éå£ï¼çèå¤§å®¶ã\n",
        "\n",
        "æ´é¢¨é¨æçºäºä¸å¤ï¼ä½è¾²æ°åä»åçåç©å¨ç©åè£¡å¾å®å¨ãæ©ä¸ï¼å¤ªé½åºä¾äºï¼æ´é¢¨é¨ä¹éå»äºãè¾²æ°æéç©åçéï¼åç©åå¿«æ¨èªç±å°èµ°åºå»ã\n",
        "\n",
        "è¾²æ°åä¿®å¥½äºæµæ¬ï¼ææäºç©åãä»åä¸èµ·å·¥ä½ï¼äºç¸å¹«å©ï¼ä¹å¹«å©ä»åçåç©ãä¸å¤©çµææï¼ä»ååå¨ä¸æ£µå¤§æ¨¹ä¸ï¼çèæ¥è½ãå·´è¿ªèººå¨ä»åè³éï¼åç©åå¨éè¿åèã\n",
        "\n",
        "æå­è£¡å¤ªå¹³äºï¼è¾²å¤«åé½å¹¸ç¦äºãä»åç¥éï¼åªè¦ç§é¡§å¥½èªå·±çåç©åå½¼æ­¤ï¼ä»åå°±ææ°¸é éèç¾å¥½ççæ´»ã\n",
        "\n",
        "å°±éæ¨£ï¼ä»åå¾æ­¤éä¸äºå¹¸ç¦ççæ´»ã\n",
        "'''\n",
        "}"
      ],
      "metadata": {
        "id": "5VFqB-ANO3PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As another useful illustration, let's plot histograms of character lengths of individual tokens for each of the languages:"
      ],
      "metadata": {
        "id": "07EiZtIe3BkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#from transformers import AutoTokenizer\n",
        "\n",
        "def plot_token_length_histogram(text, language_name):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-4o')\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    #tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "    #print(tokenizer.tokenize(text))\n",
        "\n",
        "    decoded_tokens = [encoding.decode([t]) for t in tokens]\n",
        "    token_lengths = [len(t) for t in decoded_tokens]\n",
        "    plt.hist(token_lengths, bins=10)\n",
        "    plt.title(f'Token Length Distribution for {language_name}')\n",
        "    plt.xlim([0,15])\n",
        "    plt.ylim([0,200])\n",
        "\n",
        "def plot_word_length_histogram(text, language_name):\n",
        "    if language != 'Chinese':\n",
        "        word_lengths = [len(t) for t in text.split()]\n",
        "        plt.hist(word_lengths, bins=10)\n",
        "        plt.title(f'Word Length Distribution for {language_name}')\n",
        "        plt.xlim([0,15])\n",
        "        plt.ylim([0,75])\n",
        "\n",
        "plt.figure(figsize=(20, 9))\n",
        "\n",
        "for i, language in enumerate(['English', 'French', 'Swahili', 'Chinese']):\n",
        "    # Token length histograms\n",
        "    plt.subplot(2, 4, 1 + i)\n",
        "    plot_token_length_histogram(excerpts[language], language)\n",
        "\n",
        "    # Word length histograms\n",
        "    plt.subplot(2, 4, 5 + i)\n",
        "    plot_word_length_histogram(excerpts[language], language)\n"
      ],
      "metadata": {
        "id": "57uLjAolOHZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which peculiarities do you observe?"
      ],
      "metadata": {
        "id": "bWqKis9d5F1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3.** Check how the tokenizer splits the Chinese text. You will find that some of the tokens are strange and have no clear correspondence in the text. Why do we have such tokens?"
      ],
      "metadata": {
        "id": "Mrv8cTKar4x4"
      }
    }
  ]
}